第9章 線形回帰とロジスティクス回帰

線形回帰
n個の点があるとき線形回帰はこれらの点に最も近似的、すなわちフィットする直線を探す。
線形回帰を行う理由は以下
・単純化と圧縮

線形回帰と双対性
回帰と連立方程式の解の求め方は関連がある。
連立方程式を解くときには与えられたn本のすべての線に含まれる1個の点を探す。
回帰では逆にn個の点をすべて含む1本の線を探す

線形回帰における誤差
点の集合にフィットさせた直線との間の残差とは予測された値と実際の値との差
最小二乗回帰はすべての点の残差の2乗の総和が最小となるようにする。

最適なフィッティングの探索
線形回帰ではすべての訓練点の残差の2乗の総和が最小になるような直線を探す。

より良い回帰モデル
外れ値の除去
線形回帰＝次の最小にする係数ベクトルwを探す
Σ(y-f(x))^2
残差の重みを2乗によるので外れ値の点が回帰に大きな影響を及ぼす。

非線形関数への回帰
線形回帰は実際に線形な関係があるデータにフィットさせるととても効果的
しかし一般的に面白い関数の中で完全に線形なものはない。
サポートベクターマシンなどより高度な学習メソッドの利点の1つは
明示的に数え上げなくても非線形項を組み込めること。

特徴とターゲットのスケーリング
原則として線形回帰は任意のデータセットに最もよく合う線形モデルを見つけられる。
無理やりフィットさせたモデルは次のような理由から好ましくない
・係数が読みにくい
・数値の精度の低さ
・不適切な定式化

強い相関がある特徴の処理
ターゲットと強い相関のある特徴があると予測力の高いモデルを作成できる。
ただし互いに強く相関している複数の特徴があるのは問題。

パラメータフィッティング問題としての回帰
線形回帰の式は問題がありコンピューターで扱うには向いていない。
しかし線形回帰問題には実用的により優れていることが実証された別の考え方と別の解法がある。
線形回帰をパラメータフィッティング問題としてモデリングし探索アルゴリズムを使って
パラメータの最良の値を見つける方法。

凸パラメータ空間
結論として損失関数Ｊ(ω0,ω1)は(ω0,ω1)空間の面を定義しz=Ｊ(ω0,ω1)として
その空間の中でzが最も小さくなる点を探さなければいけない。
あらゆる凸曲面には局所的な最小値が1つだけあり、さらにどの凸曲面でもその最小値を探すのは
非常に簡単であり最小値に出会うまでに下に向かって進んでいけばよい。

勾配降下探索
凸関数の最小値は任意の点から出発して下に向かって繰り返し歩みを進めていけば見つけられる。
これ以上進めない点は1つしかない。

正しい学習率はどのくらいか
損失関数の導関数は回帰問題を解くパラメータを特定する最小値に到達するのにはどの方向に移動すれば良いかわかる。
しかしどのくらい進まなければならないかはわからない。
勾配降下探索には手続きがある。
→最良の向きを見つけて一歩進むという手続きをターゲットにつくまで繰り返す。
一歩の大きさ＝学習率
学習率が高いと最小値を通り過ぎる、低いとなかなか届かない。

確率的勾配降下法
確率勾配降下法は少数の訓練データのブロックをランダムにサンプリングしそのブロックを使って
現在の位置の導関数を推定する最適化アプローチ。
ブロックサイズが小さければ小さいほど早く評価できるが推定された方向が正しいかは疑わしくなる。
凸関数では勾配降下法の学習率とブロックサイズを最適化すると処理が非常に高速化され
ライブラリ関数呼び出しによってうまく隠す方法がある。

正則化によるモデルの単純化
線形回帰はm-1個の独立変数とターゲット値によって定義されたn個の点の集合に対して
考えられる最適な回帰式を判定してくれる。

リッジ回帰
正則化は目的関数に副次的な項を追加して係数を小さく保つモデルを有利にする。
係数の2乗の総和をペナルティにするものをリッジ回帰、チコノフ正則化と呼ぶ。

Lasso回帰
リッジ回帰は小さな係数を選ぶ方向に最適化する。
2乗の総和コスト関数を使うので最も大きな係数はとりわけ大きなペナルティになる。
Lasso回帰は係数のL2ノルムを最小化するリッジ回帰とは異なり係数L1ノルムを最小化する。

フィッティングと複雑さのトレードオフ
少ないパラメータで訓練データによくフィットするなら多くのパラメータを使って
それよりもわずかによくフィットするよりも頑健なモデルなる。
このトレードオフをどう調節するかの指標は赤池情報量規準とベイズ情報量規準

分類とロジスティクス回帰
分類は与えられた入力レコードの正しいラベルを予測する問題。

分類のための回帰
訓練データのクラス名を数値に変換すれば分類問題に線形回帰を応用できる。

決定境界
分類について正しい考慮するということは特徴空間をいくつかの領域で分割するということ。
その結果ある領域内にあるすべての点は同じラベルが付与されるようになる。

ロジスティク分類の問題
バランスの取れた訓練データ
バランスの良い分類器を作るためには以下のことに注意する必要がある
・大きい方のクラスの訓練データを減らし強制的にクラスのバランスをとる
・出来れば少し揺らぎを与えながら小さいほうのクラスの要素を複製する
・大きなクラスのものよりもまれなクラスの訓練データに比重を置く

マルチクラス分類
分類問題は3つ以上のラベルから適切なものを選ばなければならないことが多い。

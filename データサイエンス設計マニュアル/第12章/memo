第12章 ビッグデータ：スケールを追求

ビッグデータとは
データの大きいものをビッグデータと呼ぶ。
どの程度の大きさかは億以上。

バッドデータとしてのビッグデータ
巨大なデータセットが作られるのは設計によるものでなく一般的には機会が得られたことによる結果
バッドデータとしては以下
・代表性の欠如
・スパムや機械が生成したコンテンツ
・冗長すぎる
・時間的なバイアスの影響を受けやすい
ビッグデータの特徴は以下
・量
・多様性
・速度

ビッグデータを扱うアルゴリズム
計算量分析
伝統的なアルゴリズム分析はランダムアクセスマシンと呼ばれる抽象的なコンピューターを基礎としている。
ハッシング
ハッシングは2次式アルゴリズムを線形時間アルゴリズムに変え操作対象の大規模データを扱えるようにするためによく使われる。
記憶階層構造の活用
ビッグデータアルゴリズムはCPUバウンドではなくI/Oや帯域幅バウンドになることが多い。

フィルタリングとサンプリング
フィルタリングは特定の基準に基づきデータの中でも重要な部分を選択する。
サンプリングは対象領域固有の基準など使わず適切なサイズの部分集合を無作為に選ぶ
両方ともビッグデータを利用する上で破棄する方法

